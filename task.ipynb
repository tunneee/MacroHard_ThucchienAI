{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0080b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Collecting sortedcontainers (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from webdriver-manager) (2.32.5)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/myvenv/lib/python3.10/site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Downloading selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, python-dotenv, pysocks, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [selenium]8/9\u001b[0m [selenium]tenv]\n",
      "\u001b[1A\u001b[2KSuccessfully installed outcome-1.3.0.post0 pysocks-1.7.1 python-dotenv-1.1.1 selenium-4.38.0 sortedcontainers-2.4.0 trio-0.31.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580bec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin, urlparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5004edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting links from https://vneconomy.vn/hon-2-trieu-khach-hang-da-bat-sinh-loi-tu-dong-cung-techcombank.htm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 links to crawl.\n",
      "Crawling https://vneconomy.vn/hon-2-trieu-khach-hang-da-bat-sinh-loi-tu-dong-cung-techcombank.htm...\n",
      "Crawling https://vneconomy.vn/chung-khoan.htm...\n",
      "Finished crawling https://vneconomy.vn/hon-2-trieu-khach-hang-da-bat-sinh-loi-tu-dong-cung-techcombank.htm. Text length: 7291\n",
      "Finished crawling https://vneconomy.vn/chung-khoan.htm. Text length: 6312\n",
      "\n",
      "--- Crawling Complete ---\n",
      "Data saved to crawled_text.txt\n",
      "Total execution time: 24.98 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_links_from_url(url, max_links=50):\n",
    "    \"\"\"\n",
    "    Connects to a URL and extracts up to max_links of unique, valid URLs found on the page.\n",
    "    \"\"\"\n",
    "    print(f\"Getting links from {url}...\")\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = None\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for dynamic content to load\n",
    "\n",
    "        links = set()\n",
    "        elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        for element in elements:\n",
    "            if len(links) >= max_links:\n",
    "                break\n",
    "            href = element.get_attribute('href')\n",
    "            if href:\n",
    "                absolute_url = urljoin(url, href)\n",
    "                parsed_url = urlparse(absolute_url)\n",
    "                if parsed_url.scheme in ['http', 'https'] and parsed_url.netloc:\n",
    "                    links.add(absolute_url)\n",
    "        return list(links)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while getting links from {url}: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "def crawl_website(url):\n",
    "    \"\"\"\n",
    "    Crawls a single website and returns its main text content.\n",
    "    Each call creates its own WebDriver instance for thread safety.\n",
    "    \"\"\"\n",
    "    print(f\"Crawling {url}...\")\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = None\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        body = driver.find_element(By.TAG_NAME, 'body')\n",
    "        text = body.text\n",
    "        print(f\"Finished crawling {url}. Text length: {len(text)}\")\n",
    "        return f\"--- Content from {url} ---\\n{text}\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Could not crawl {url}: {e}\")\n",
    "        return f\"--- Could not crawl {url} ---\\nError: {e}\\n\\n\"\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "# --- Main Execution ---\n",
    "start_time = time.time()\n",
    "main_url = \"https://vneconomy.vn/hon-2-trieu-khach-hang-da-bat-sinh-loi-tu-dong-cung-techcombank.htm\"\n",
    "\n",
    "# 1. Get up to 50 links from the main page.\n",
    "initial_links = get_links_from_url(main_url, max_links=1)\n",
    "\n",
    "if not initial_links:\n",
    "    print(\"No links found. Exiting.\")\n",
    "else:\n",
    "    print(f\"Found {len(initial_links)} links to crawl.\")\n",
    "    \n",
    "    # We will crawl the main URL as well as the links found.\n",
    "    urls_to_crawl = [main_url] + initial_links\n",
    "    # Ensure we don't exceed 50 \"relative\" sites + 1 main site.\n",
    "    urls_to_crawl = urls_to_crawl[:51] \n",
    "\n",
    "    all_text = \"\"\n",
    "\n",
    "    # 2. Crawl websites in parallel using a thread pool.\n",
    "    # The number of workers can be adjusted based on your system's resources.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_url = {executor.submit(crawl_website, url): url for url in urls_to_crawl}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            try:\n",
    "                data = future.result()\n",
    "                all_text += data\n",
    "            except Exception as exc:\n",
    "                url_for_exc = future_to_url[future]\n",
    "                print(f'{url_for_exc} generated an exception: {exc}')\n",
    "\n",
    "    # 3. Save the result to a single text file.\n",
    "    output_filename = \"crawled_text.txt\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_text)\n",
    "        \n",
    "    print(\"\\n--- Crawling Complete ---\")\n",
    "    print(f\"Data saved to {output_filename}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1765531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to crawl https://uxfoundation.vn/bai-viet/tinh-nang-sinh-loi-tu-dong-cua-techcombank...\n",
      "Successfully crawled and saved content to 'techcombank_sinh_loi_tu_dong.txt'\n",
      "Total characters saved: 7337\n",
      "Browser closed.\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def crawl_and_save(url, output_filename):\n",
    "    \"\"\"\n",
    "    Crawls a single website using Selenium, extracts its main text,\n",
    "    and saves it to a specified text file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to crawl.\n",
    "        output_filename (str): The name of the file to save the text to.\n",
    "    \"\"\"\n",
    "    print(f\"Starting to crawl {url}...\")\n",
    "    \n",
    "    # Setup headless Chrome options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run browser in the background\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    driver = None  # Initialize driver to None\n",
    "    \n",
    "    try:\n",
    "        # Automatically manage ChromeDriver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        # Navigate to the URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait a few seconds for dynamic content to load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find the body element and extract all visible text\n",
    "        body_element = driver.find_element(By.TAG_NAME, 'body')\n",
    "        content = body_element.text\n",
    "        \n",
    "        # Save the extracted content to a file\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "            \n",
    "        print(f\"Successfully crawled and saved content to '{output_filename}'\")\n",
    "        print(f\"Total characters saved: {len(content)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while crawling {url}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Ensure the browser is closed even if an error occurs\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"Browser closed.\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# You can replace this with any URL and filename you want.\n",
    "target_url = \"https://uxfoundation.vn/bai-viet/tinh-nang-sinh-loi-tu-dong-cua-techcombank\"\n",
    "output_file = \"techcombank_sinh_loi_tu_dong.txt\"\n",
    "\n",
    "crawl_and_save(target_url, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec9e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
